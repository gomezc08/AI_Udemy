{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gomezc08/AI_Udemy/blob/main/Deep_Convolutional_Q_Learning_Pac_Man.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Convolutional Q-Learning for Pac-Man"
      ],
      "metadata": {
        "id": "EAiHVEoWHy_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 0 - Installing the required packages and importing the libraries"
      ],
      "metadata": {
        "id": "tjO1aK3Ddjs5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Gymnasium"
      ],
      "metadata": {
        "id": "NwdRB-ZLdrAV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbnq3XpoKa_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3899c01-4de3-45f6-9d81-ae69507c7a76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "\u001b[33mWARNING: gymnasium 1.1.1 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license,atari]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license,atari]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license,atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Requirement already satisfied: ale_py>=0.9 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license,atari]) (0.10.2)\n",
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.11/dist-packages (0.10.2)\n",
            "Requirement already satisfied: numpy>1.20 in /usr/local/lib/python3.11/dist-packages (from ale-py) (2.0.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 2s (739 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 126209 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2351178 sha256=8e083068cacfa13e445e98449d8bdbad1076be68f76ccfda158c35bf3cc415b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, box2d-py\n",
            "Successfully installed box2d-py-2.3.5 swig-4.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
        "!pip install ale-py\n",
        "!apt-get install -y swig\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the libraries"
      ],
      "metadata": {
        "id": "H-wes4LZdxdd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho_25-9_9qnu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1 - Building the AI"
      ],
      "metadata": {
        "id": "m7wa0ft8e3M_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the architecture of the Neural Network"
      ],
      "metadata": {
        "id": "dlYVpVdHe-i6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "  def __init__(self, action_size, seed = 42):\n",
        "    super(Network, self).__init__()   # inheritance purposes (inheriting from nn.module so need to ensure we bring in appropriate methods).\n",
        "    self.seed = torch.manual_seed(seed)\n",
        "    self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)\n",
        "    self.bn1 = nn.BatchNorm2d(32)\n",
        "    self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "    self.bn2 = nn.BatchNorm2d(64)\n",
        "    self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "    self.bn3 = nn.BatchNorm2d(64)\n",
        "    self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1)\n",
        "    self.bn4 = nn.BatchNorm2d(128)\n",
        "    self.fc1 = nn.Linear(10 * 10 * 128, 512)     # 10x10 image with 128 features.\n",
        "    self.fc2 = nn.Linear(512, 256)\n",
        "    self.fc3 = nn.Linear(256, action_size)\n",
        "\n",
        "  # state - images of our initial state.\n",
        "  def forward(self, state):\n",
        "    x = F.relu(self.bn1(self.conv1(state)))   # find the conv of the state and then we will normalize it and then activate it.\n",
        "    x = F.relu(self.bn2(self.conv2(x)))\n",
        "    x = F.relu(self.bn3(self.conv3(x)))\n",
        "    x = F.relu(self.bn4(self.conv4(x)))\n",
        "    x = x.view(x.size(0), -1)   # similar to flatten; RESHAPES TENSOR: takes the size of x from previous line and flattens.\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    return self.fc3(x)"
      ],
      "metadata": {
        "id": "uIimOOGd8jXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2 - Training the AI"
      ],
      "metadata": {
        "id": "rUvCfE_mhwo2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the environment"
      ],
      "metadata": {
        "id": "WWCDPF22lkwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ale_py\n",
        "import gymnasium as gym\n",
        "# deterministic means the ghost will make deterministic moves (won't move intelligently).\n",
        "env = gym.make('MsPacmanDeterministic-v0', full_action_space = False)   # only 9 action space vs 18.\n",
        "state_shape = env.observation_space.shape\n",
        "state_size = env.observation_space.shape[0]\n",
        "number_actions = env.action_space.n\n",
        "print('State shape: ', state_shape)   # 210 x 160 images with 3 channels; we will resize this in the preprocess frames.\n",
        "print('State size: ', state_size)\n",
        "print('Number of actions: ', number_actions)"
      ],
      "metadata": {
        "id": "dx8WQ7lbeY_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5ec663c-d6d3-4598-d2d6-9fd3b2a04af4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State shape:  (210, 160, 3)\n",
            "State size:  210\n",
            "Number of actions:  9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gymnasium/envs/registration.py:519: DeprecationWarning: \u001b[33mWARN: The environment MsPacmanDeterministic-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the hyperparameters"
      ],
      "metadata": {
        "id": "Bx6IdX3ciDqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 5e-4                # learning rate: how much model updates parameter (gradient descent step size)\n",
        "batch_size = 64             # updating parameters in batches of 100\n",
        "gamma = 0.99                # discount factor: how to reward agent; [0,1] -> [immediate reward, future rewards]"
      ],
      "metadata": {
        "id": "AREIcjKxw7iV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing the frames"
      ],
      "metadata": {
        "id": "U2bDShIEkA5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# turn the convolution tensors to vector tensors - so it can be fed to network of AI\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "def preprocess_frame(frame):\n",
        "  # array to PIL image.\n",
        "  frame = Image.fromarray(frame)\n",
        "\n",
        "  # PIL image -> tensor.\n",
        "  preprocess_object = transforms.Compose([transforms.Resize((128,128)), transforms.ToTensor()])     # working with 128x128 images from pacman.\n",
        "  return preprocess_object(frame).unsqueeze(0)"
      ],
      "metadata": {
        "id": "16xByREcy_zZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing the DCQN class"
      ],
      "metadata": {
        "id": "imMdSO-HAWra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Agent - defines behaviour of our agent as it interacts with the enviornment.\n",
        "# act - selects an action based on q-value from network while taking into account epsilon (go through network and get q-value).\n",
        "# learn - calculates target and expected q-value (using equation) and learns because we wanna minimize our loss function.\n",
        "# step -  store experiences and decide when to learn from them (every 4 steps).\n",
        "\n",
        "class Agent():\n",
        "  # local_qnetwork - the network we are actually doing training on (where we are grabbing our q-values from and teaching agent to move).\n",
        "  # target_qnetwork - used for computing the loss (actual q-value).\n",
        "\n",
        "  def __init__(self, action_size):\n",
        "     self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")       # using gpu.\n",
        "     self.action_size = action_size\n",
        "     self.local_qnetwork = Network(action_size).to(self.device)\n",
        "     self.target_qnetwork = Network(action_size).to(self.device)\n",
        "     self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr = alpha)\n",
        "     self.memory = deque(maxlen = 1000)\n",
        "\n",
        "  # store experiences and decide when to learn from them (every 4 steps).\n",
        "  def step(self, state, action, reward, next_state, done_status):\n",
        "    # preprocess.\n",
        "    state = preprocess_frame(state)\n",
        "    next_state = preprocess_frame(next_state)\n",
        "    # append experience to memory.\n",
        "    self.memory.append((state, action, reward, next_state, done_status))\n",
        "    # ensure we have enough to make batches of memory.\n",
        "    if len(self.memory) > batch_size:\n",
        "      experience_batch = random.sample(self.memory, k = batch_size)\n",
        "      self.learn(experience_batch, gamma)\n",
        "\n",
        "  # selecting an action (highest q value) based on the current state + epsilon.\n",
        "  def act(self, state, epsilon = 0.):\n",
        "    state = preprocess_frame(state).to(self.device)      # adding a batch to the state to show which batch the state belongs to.\n",
        "    self.local_qnetwork.eval()    # going from training mode -> evaluation mode (kinda like an isoloated evaluation environment).\n",
        "    # ensure we are using evaluation mode before grabbing our predicted q values.\n",
        "    with torch.no_grad():\n",
        "      action_values = self.local_qnetwork(state)\n",
        "\n",
        "    self.local_qnetwork.train()  # going from evaluation mode -> training mode.\n",
        "    if random.random() > epsilon:\n",
        "      return np.argmax(action_values.cpu().data.numpy())\n",
        "    else:\n",
        "      return random.choice(np.arange(self.action_size))\n",
        "\n",
        "  # updating q-values from our local network to better match those of the target network.\n",
        "  \"\"\"def learn(self, experiences, discount_factor):\n",
        "    # unpack our experiences.\n",
        "    states, actions, rewards, next_states, dones = zip(*experiences)\n",
        "\n",
        "    # Remove extra batch dimension before stacking\n",
        "    states = torch.cat([s.squeeze(0) for s in states]).float().to(self.device)\n",
        "    next_states = torch.cat([s.squeeze(0) for s in next_states]).float().to(self.device)\n",
        "\n",
        "    # Reshape to (batch_size, channels, height, width)\n",
        "    states = states.view(batch_size, 3, 128, 128)\n",
        "    next_states = next_states.view(batch_size, 3, 128, 128)\n",
        "\n",
        "    actions = torch.from_numpy(np.vstack([actions])).long().to(self.device)\n",
        "    rewards = torch.from_numpy(np.vstack([rewards])).float().to(self.device)\n",
        "    dones = torch.from_numpy(np.vstack([dones])).to(torch.uint8).float().to(self.device)\n",
        "\n",
        "    Q_target_next = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "    Q_target = rewards + (discount_factor * Q_target_next * (1 - dones))    # Q_t equation.\n",
        "    Q_expected = self.local_qnetwork(states).gather(1, actions)\n",
        "    loss = F.mse_loss(Q_expected, Q_target)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()   # back propogates.\n",
        "    self.optimizer.step()  \"\"\"                                      # updates local model's parameters.\n",
        "\n",
        "  def learn(self, experiences, discount_factor):\n",
        "    states, actions, rewards, next_states, dones = zip(*experiences)\n",
        "    states = torch.from_numpy(np.vstack(states)).float().to(self.device)\n",
        "    actions = torch.from_numpy(np.vstack(actions)).long().to(self.device)\n",
        "    rewards = torch.from_numpy(np.vstack(rewards)).float().to(self.device)\n",
        "    next_states = torch.from_numpy(np.vstack(next_states)).float().to(self.device)\n",
        "    dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(self.device)\n",
        "    next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "    q_targets = rewards + discount_factor * next_q_targets * (1 - dones)\n",
        "    q_expected = self.local_qnetwork(states).gather(1, actions)\n",
        "    loss = F.mse_loss(q_expected, q_targets)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()"
      ],
      "metadata": {
        "id": "rRO8Whb3JjCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the DCQN agent"
      ],
      "metadata": {
        "id": "yUg95iBpAwII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(action_size=number_actions)"
      ],
      "metadata": {
        "id": "zLnelwiJOqai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize Hyperprarameters to Training"
      ],
      "metadata": {
        "id": "SDf_XtWgXsGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 2000\n",
        "maximum_number_timesteps_per_episode = 10000    # max number of steps for the rocket to reach surface (dont want AI to train too hard).\n",
        "# epsilon = exploitation vs exploration; 1 = exploration, 0 = exploitation.\n",
        "epsilon_starting_value = 1.0\n",
        "epsilon_ending_value = 0.01\n",
        "epsilon_decay_value = 0.995\n",
        "epsilon = epsilon_starting_value\n",
        "scores_100_episodes = deque(maxlen=100)   # window that keeps track of the scores of last 100 episodes."
      ],
      "metadata": {
        "id": "b36UGzlMXxpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the DCQN agent"
      ],
      "metadata": {
        "id": "CK6Zt_gNmHvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "  state, _ = env.reset()    # since we are starting a new episode, we gotta reset the env everytime (reset rocket ship). _ ensures we dont care abt any return value.\n",
        "  score = 0   # cummulative reward for entire episode (wanna maximize this).\n",
        "  for t in range(maximum_number_timesteps_per_episode):\n",
        "    action = agent.act(state, epsilon)\n",
        "    next_state, reward, done, _, _ = env.step(action)   # built in.\n",
        "    agent.step(state, action, reward, next_state, done)\n",
        "    state = next_state\n",
        "    score += reward\n",
        "    # check if episode is done or not.\n",
        "    if done:\n",
        "      break\n",
        "  scores_100_episodes.append(score)\n",
        "  epsilon = max(epsilon_ending_value, epsilon * epsilon_decay_value)\n",
        "  print('\\rEpoch {}\\tAverage Score: {:.2f}'.format(epoch, np.mean(scores_100_episodes)), end = \"\")\n",
        "  if epoch % 100 == 0:\n",
        "    print('\\rEpoch {}\\tAverage Score: {:.2f}'.format(epoch, np.mean(scores_100_episodes)), end = \"\")\n",
        "    # score for pacman is a score of 500 (coins eaten)\n",
        "  if np.mean(scores_100_episodes) >= 500.0:\n",
        "    print('\\nEnviornment solved in {:d}\\tAverage Score: {:.2f}'.format(epoch, np.mean(scores_100_episodes)), end = \"\")\n",
        "    torch.save(agent.local_qnetwork.state_dict(), 'checkpoint.pth')\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UlWMyzRXhYX",
        "outputId": "19469482-bfd3-4851-9d46-87676629a223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 84\tAverage Score: 380.60"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3 - Visualizing the results"
      ],
      "metadata": {
        "id": "-0WhhBV8nQdf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb9nVvU2Okhk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "0708937f-53a0-45f9-fc7b-732bfd53ba4e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'agent' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6261a2f96b76>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mimageio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'video.mp4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mshow_video_of_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MsPacmanDeterministic-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshow_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import imageio\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def show_video_of_model(agent, env_name):\n",
        "    env = gym.make(env_name, render_mode='rgb_array')\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    frames = []\n",
        "    while not done:\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "        action = agent.act(state)\n",
        "        state, reward, done, _, _ = env.step(action)\n",
        "    env.close()\n",
        "    imageio.mimsave('video.mp4', frames, fps=30)\n",
        "\n",
        "show_video_of_model(agent, 'MsPacmanDeterministic-v0')\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob.glob('*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "show_video()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}